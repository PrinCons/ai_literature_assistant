{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WIP at the moment; use research_ver.ipynb for results",
   "id": "547572a9c61d1a56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Literature Review Assistant\n",
    "Goal: this notebook provides an LLM \"research assistant\" that can help with exploring the vaguest of questions by providing the relevant papers on the matter. It does not read the papers for you, but it provides a short summary that can be more helpful than the standard abstract when checking out several papers in one sitting. \n",
    "Secondary Goal: this \"assistant\" helps the exploration of sub-topics and alternative keywords; this is helpful when the question is vague or lacks crucial keywords, or when the topic is vast. \n",
    " "
   ],
   "id": "7c0018c510056e83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:10.093758Z",
     "start_time": "2025-04-17T22:25:10.086506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### set your query \n",
    "query = \"I am looking for a paper where an LLM managed to translate an unfamiliar language after being shown the vocabulary in the prompt \"\n",
    "### set the desired number of subtopics\n",
    "number_of_subtopics = 5"
   ],
   "id": "88854918cc9ebdf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:12.451751Z",
     "start_time": "2025-04-17T22:25:10.105808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from literature_review_assistant import ai_research_assistant\n",
    "from IPython.display import Markdown"
   ],
   "id": "fc9c1051bddc498f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:42.643248Z",
     "start_time": "2025-04-17T22:25:12.538553Z"
    }
   },
   "cell_type": "code",
   "source": "r = ai_research_assistant(query, number_of_subtopics)",
   "id": "160d1b91649500d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a summary of each article:\\n\\n**Main Article:**\\n\\n*   **Title:** Killing it with Zero-Shot: Adversarially Robust Novelty Detection\\n*   **Authors:** Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, Mohammad Hossein Rohban\\n*   **ID:** http://arxiv.org/abs/2501.15271v1\\n*   **Summary:** This paper addresses the vulnerability of novelty detection (ND) algorithms to adversarial attacks. It proposes a method that combines nearest-neighbor algorithms with robust features from ImageNet-pretrained models to enhance the robustness and performance of ND. The approach demonstrates significant improvements over state-of-the-art methods under adversarial conditions, establishing a new standard for robust ND. The implementation is publicly available.\\n\\n**Other Articles:**\\n\\n*   **Title:** Augmenting Large Language Model Translators via Translation Memories\\n*   **Authors:** Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu\\n*   **ID:** http://arxiv.org/abs/2305.17367v1\\n*   **Summary:** This paper explores using translation memories (TMs) as prompts to improve the in-context learning of machine translation models, specifically large language models (LLMs). The results show that LLMs can leverage high-quality TM-based prompts to achieve translation performance comparable to state-of-the-art NMT systems.\\n\\n*   **Title:** Human-in-the-loop Machine Translation with Large Language Model\\n*   **Authors:** Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao\\n*   **ID:** http://arxiv.org/abs/2310.08908v1\\n*   **Summary:** This paper proposes a human-in-the-loop pipeline to guide LLMs in producing customized translations with revision instructions. The pipeline uses both automatic retrieval and human feedback to enhance the LLM\\'s translation through in-context learning, demonstrating effectiveness in tailoring in-domain translations.\\n\\n*   **Title:** Adaptive Machine Translation with Large Language Models\\n*   **Authors:** Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way\\n*   **ID:** http://arxiv.org/abs/2301.13294v3\\n*   **Summary:** This paper investigates the use of in-context learning to improve real-time adaptive machine translation with LLMs. The experiments show that LLMs can adapt to in-domain sentence pairs and terminology, surpassing strong encoder-decoder MT systems, especially for high-resource languages.\\n\\n*   **Title:** Translate-and-Revise: Boosting Large Language Models for Constrained Translation\\n*   **Authors:** Pengcheng Huang, Yongyu Mu, Yuzhang Wu, Bei Li, Chunyang Xiao, Tong Xiao, Jingbo Zhu\\n*   **ID:** http://arxiv.org/abs/2407.13164v1\\n*   **Summary:** This paper introduces a \"translate-and-revise\" approach to improve constrained translation with LLMs. By adding a revision process that prompts LLMs about unmet constraints, the approach achieves significant improvements in constraint-based translation accuracy compared to standard LLMs and NMT methods.\\n\\n\\n\\n----some helpful keywords/topics may be: In-context learning for low-resource languages\\nZero-shot translation with large language models\\nFew-shot translation with large language models\\nLLM adaptation to novel languages\\nPrompt engineering for machine translation\\n\\n\\nHere\\'s a summary of how the provided articles relate to using LLMs for translation, particularly with a focus on prompt engineering and iterative refinement:\\n\\n*   **Iterative Translation Refinement with Large Language Models** by Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield (http://arxiv.org/abs/2306.03856v2): This paper directly addresses improving translation quality by iteratively prompting an LLM to refine its own translations. This relates to the researcher\\'s interest because it explores how to leverage LLMs\\' capabilities through prompting strategies to achieve better translation outcomes.\\n\\n*   **Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT\\'s Customizability** by Masaru Yamada (http://arxiv.org/abs/2308.01391v2): This paper investigates how prompt engineering, specifically including the purpose of the translation and target audience, impacts the quality of translations produced by ChatGPT. This is relevant because it demonstrates how carefully crafted prompts can significantly influence the output of LLMs in translation tasks, potentially enabling them to handle nuances and specific requirements.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:42.851185Z",
     "start_time": "2025-04-17T22:25:42.842495Z"
    }
   },
   "cell_type": "code",
   "source": "Markdown(r)",
   "id": "934fd169c0372856",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Here's a summary of each article:\n\n**Main Article:**\n\n*   **Title:** Killing it with Zero-Shot: Adversarially Robust Novelty Detection\n*   **Authors:** Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, Mohammad Hossein Rohban\n*   **ID:** http://arxiv.org/abs/2501.15271v1\n*   **Summary:** This paper addresses the vulnerability of novelty detection (ND) algorithms to adversarial attacks. It proposes a method that combines nearest-neighbor algorithms with robust features from ImageNet-pretrained models to enhance the robustness and performance of ND. The approach demonstrates significant improvements over state-of-the-art methods under adversarial conditions, establishing a new standard for robust ND. The implementation is publicly available.\n\n**Other Articles:**\n\n*   **Title:** Augmenting Large Language Model Translators via Translation Memories\n*   **Authors:** Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu\n*   **ID:** http://arxiv.org/abs/2305.17367v1\n*   **Summary:** This paper explores using translation memories (TMs) as prompts to improve the in-context learning of machine translation models, specifically large language models (LLMs). The results show that LLMs can leverage high-quality TM-based prompts to achieve translation performance comparable to state-of-the-art NMT systems.\n\n*   **Title:** Human-in-the-loop Machine Translation with Large Language Model\n*   **Authors:** Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao\n*   **ID:** http://arxiv.org/abs/2310.08908v1\n*   **Summary:** This paper proposes a human-in-the-loop pipeline to guide LLMs in producing customized translations with revision instructions. The pipeline uses both automatic retrieval and human feedback to enhance the LLM's translation through in-context learning, demonstrating effectiveness in tailoring in-domain translations.\n\n*   **Title:** Adaptive Machine Translation with Large Language Models\n*   **Authors:** Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way\n*   **ID:** http://arxiv.org/abs/2301.13294v3\n*   **Summary:** This paper investigates the use of in-context learning to improve real-time adaptive machine translation with LLMs. The experiments show that LLMs can adapt to in-domain sentence pairs and terminology, surpassing strong encoder-decoder MT systems, especially for high-resource languages.\n\n*   **Title:** Translate-and-Revise: Boosting Large Language Models for Constrained Translation\n*   **Authors:** Pengcheng Huang, Yongyu Mu, Yuzhang Wu, Bei Li, Chunyang Xiao, Tong Xiao, Jingbo Zhu\n*   **ID:** http://arxiv.org/abs/2407.13164v1\n*   **Summary:** This paper introduces a \"translate-and-revise\" approach to improve constrained translation with LLMs. By adding a revision process that prompts LLMs about unmet constraints, the approach achieves significant improvements in constraint-based translation accuracy compared to standard LLMs and NMT methods.\n\n\n\n----some helpful keywords/topics may be: In-context learning for low-resource languages\nZero-shot translation with large language models\nFew-shot translation with large language models\nLLM adaptation to novel languages\nPrompt engineering for machine translation\n\n\nHere's a summary of how the provided articles relate to using LLMs for translation, particularly with a focus on prompt engineering and iterative refinement:\n\n*   **Iterative Translation Refinement with Large Language Models** by Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield (http://arxiv.org/abs/2306.03856v2): This paper directly addresses improving translation quality by iteratively prompting an LLM to refine its own translations. This relates to the researcher's interest because it explores how to leverage LLMs' capabilities through prompting strategies to achieve better translation outcomes.\n\n*   **Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability** by Masaru Yamada (http://arxiv.org/abs/2308.01391v2): This paper investigates how prompt engineering, specifically including the purpose of the translation and target audience, impacts the quality of translations produced by ChatGPT. This is relevant because it demonstrates how carefully crafted prompts can significantly influence the output of LLMs in translation tasks, potentially enabling them to handle nuances and specific requirements.\n"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:42.961787Z",
     "start_time": "2025-04-17T22:25:42.957398Z"
    }
   },
   "cell_type": "code",
   "source": "type(r)",
   "id": "a6dd2b3b3c5d9b6b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:43.051078Z",
     "start_time": "2025-04-17T22:25:43.047731Z"
    }
   },
   "cell_type": "code",
   "source": "print(r[0])",
   "id": "3149abfd367c6fef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T22:25:43.169618Z",
     "start_time": "2025-04-17T22:25:43.166863Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6d2f033b8480471a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
