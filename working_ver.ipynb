{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WIP at the moment; use research_ver.ipynb for results",
   "id": "547572a9c61d1a56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Literature Review Assistant\n",
    "Goal: this notebook provides an LLM \"research assistant\" that can help with exploring the vaguest of questions by providing the relevant papers on the matter. It does not read the papers for you, but it provides a short summary that can be more helpful than the standard abstract when checking out several papers in one sitting. \n",
    "Secondary Goal: this \"assistant\" helps the exploration of sub-topics and alternative keywords; this is helpful when the question is vague or lacks crucial keywords, or when the topic is vast. \n",
    " "
   ],
   "id": "7c0018c510056e83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:41.189267Z",
     "start_time": "2025-04-13T19:06:54.182399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### set your query \n",
    "query = \"I am looking for a paper where an LLM managed to translate an unfamiliar language after being shown the vocabulary in the prompt \"\n",
    "### set the desired number of subtopics\n",
    "number_of_subtopics = 5"
   ],
   "id": "88854918cc9ebdf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Actual Code",
   "id": "7628926db627c10f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:41.189463Z",
     "start_time": "2025-04-13T19:06:54.207439Z"
    }
   },
   "source": [
    "from anaconda_navigator.api.external_apps.bundle.installers import retrieve_and_validate\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests \n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "# from google.api_core import retry\n",
    "import chromadb\n",
    "from nltk.corpus.reader import documents\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from numpy.array_api import result_type\n",
    "from starlette.routing import request_response"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/kjwj3q012r99v59302lxlcbc0000gn/T/ipykernel_58368/1712358717.py:22: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\n",
      "  from numpy.array_api import result_type\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.586354Z",
     "start_time": "2025-04-13T19:06:59.457217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ],
   "id": "a25053b4db706d1d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.599525Z",
     "start_time": "2025-04-13T19:06:59.596343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_model_response(prompt:str) -> str:\n",
    "    config = types.GenerateContentConfig(temperature=0.0)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        config=config,\n",
    "        contents=prompt,\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ],
   "id": "874685fce302f5a5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.612417Z",
     "start_time": "2025-04-13T19:06:59.608304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_best_query(request: str, main_query = False) -> list:\n",
    "\n",
    "    prompt = f'''You are a helpful research assistant doing a literature review. The researcher says: \"{request}\". What would be the most accurate arXiv API call to find this information? Please provide the API call alone, no need for an explanation. \n",
    "        INSTRUCTIONS: \n",
    "        Step 1 - consider how an arXiv API call is constructed. \n",
    "        Step 2 - define the best search query to use for the researcher's request\n",
    "        Step 3 - construct the arXiv API call\n",
    "        Step 4 - make sure the arXiv API call is valid, and fix it if it's not'''\n",
    "    \n",
    "    response = get_model_response(prompt)\n",
    "\n",
    "    best_query = response.strip().replace(\"\\n\", \"\")\n",
    "    if main_query:\n",
    "        best_query_pages = [best_query, best_query.replace(\"start=0\", \"start=10\"), best_query.replace(\"start=0\", \"start=20\")]\n",
    "    \n",
    "        return best_query_pages\n",
    "    \n",
    "    return best_query\n",
    "    "
   ],
   "id": "c9b3c7a2cf16c8d5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.625830Z",
     "start_time": "2025-04-13T19:06:59.621076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_subtopic_query(request: str) -> list:\n",
    "    \n",
    "    prompt=f'''You are a helpful research assistant doing a literature review. The researcher says: \"{request}\". What would be {number_of_subtopics} relevant sub-topics to gain a better understanding of this matter? Please provide a list of these topics. Provide no explanation. Return a Python list. '''\n",
    "    \n",
    "    close_topics = get_model_response(prompt)\n",
    "    json_start = close_topics.find(\"[\")\n",
    "    s = close_topics[json_start:].replace(\"`\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace('\"', \"\").replace(\"\\n\", \"\")\n",
    "    \n",
    "    subtopics = [i.strip() for i in s.split(\", \")]\n",
    "    \n",
    "    subtopic_queries = []\n",
    "    \n",
    "    for topic in subtopics:\n",
    "        subtopic_query = f\"I want to look into {topic}\"\n",
    "        r = generate_best_query(subtopic_query)\n",
    "        subtopic_queries.append(r.strip().replace(\"\\n\", \"\"))\n",
    "    \n",
    "    return  subtopics, subtopic_queries"
   ],
   "id": "66df6dd741531547",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.637924Z",
     "start_time": "2025-04-13T19:06:59.634536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_search_queries(request: str) -> list:\n",
    "    main_query = generate_best_query(request, main_query=True)\n",
    "    subtopic_results = generate_subtopic_query(request)\n",
    "    subtopic_queries = subtopic_results[1]\n",
    "    subtopic_list = subtopic_results[0]\n",
    "    return main_query + subtopic_queries, subtopic_list "
   ],
   "id": "99820a390307a254",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.661302Z",
     "start_time": "2025-04-13T19:06:59.654497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_arxiv_metadata(url: str, topic: str) -> tuple:\n",
    "    \n",
    "    arxiv_entries = {}\n",
    "    \n",
    "    url = url.replace(\" \", \"\").replace(\"`\", \"\")\n",
    "    \n",
    "    arxiv_response = requests.get(url)\n",
    "    if arxiv_response.status_code != 200:\n",
    "        raise requests.HTTPError(f\"API call failed with status code {response.status_code}\")\n",
    "        return \n",
    "    else:\n",
    "        root = ET.fromstring(arxiv_response.text)\n",
    "        ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "        \n",
    "        for entry in root.findall('atom:entry', ns):\n",
    "            title = entry.find('atom:title', ns).text.strip()\n",
    "            summary = entry.find('atom:summary', ns).text.strip()\n",
    "            authors = [author.find('atom:name', ns).text.strip()\n",
    "                        for author in entry.findall('atom:author', ns)]\n",
    "            link = entry.find('atom:link', ns).attrib['href']\n",
    "            id = entry.find('atom:id', ns).text.strip()\n",
    "            updated = entry.find('atom:updated', ns).text.strip()\n",
    "            updated = datetime.datetime.strptime(updated, '%Y-%m-%dT%H:%M:%SZ').date()\n",
    "            if id not in arxiv_entries:\n",
    "                arxiv_entries[id] = {\n",
    "                    'summary': summary,\n",
    "                    'metadatas': {\n",
    "                        'authors': \", \".join(authors),\n",
    "                        'title': title,\n",
    "                        'published_url': link,\n",
    "                        'updated': str(updated),\n",
    "                        'id_url': id,\n",
    "                        'original_search': url, \n",
    "                        'topic': topic\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "        return arxiv_entries\n",
    "        "
   ],
   "id": "6582a160614623df",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.674955Z",
     "start_time": "2025-04-13T19:06:59.670603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a helper to retry when per-minute quota is reached.\n",
    "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    # @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        response = client.models.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(\n",
    "                task_type=embedding_task,\n",
    "            ),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ],
   "id": "13e7f50584cd04e7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.767453Z",
     "start_time": "2025-04-13T19:06:59.683197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DB_NAME = \"arxiv_results\"\n",
    "\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n"
   ],
   "id": "da04f76205bd07b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/kjwj3q012r99v59302lxlcbc0000gn/T/ipykernel_58368/26373352.py:3: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embed_fn = GeminiEmbeddingFunction()\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:06:59.805930Z",
     "start_time": "2025-04-13T19:06:59.800794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_db_content(call_list: list, subtopics: list): \n",
    "    counter = 0\n",
    "    subtopic_idx = 0\n",
    "    n = len(subtopics)\n",
    "    calls = len(call_list)\n",
    "    for i in range(calls):\n",
    "        if i > calls - n - 1: \n",
    "            subtopic = subtopics[subtopic_idx]\n",
    "            subtopic_idx += 1\n",
    "        else:\n",
    "            subtopic = \"main\"\n",
    "        api_call = call_list[i]\n",
    "        response = get_arxiv_metadata(api_call, subtopic)\n",
    "        if response is not None:\n",
    "            for article_id in response:\n",
    "                db.add(documents=[response[article_id][\"summary\"]], metadatas=[response[article_id][\"metadatas\"]],  ids=[article_id])\n",
    "                counter += 1\n",
    "        "
   ],
   "id": "ff3fb1c11731935b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:06.377899Z",
     "start_time": "2025-04-13T19:06:59.815629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_result = generate_search_queries(query)\n",
    "r, st = full_result[0], full_result[1]"
   ],
   "id": "a561342ae3d12f53",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.022384Z",
     "start_time": "2025-04-13T19:07:06.387049Z"
    }
   },
   "cell_type": "code",
   "source": "generate_db_content(r, st)",
   "id": "e20a5d6839c674f3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.318274Z",
     "start_time": "2025-04-13T19:07:40.031332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_fn.document_mode = False\n",
    "\n",
    "result = db.query(query_texts=[query], n_results= 3 + 1)\n",
    "\n",
    "[all_passages] = result[\"documents\"] \n",
    "all_titles = [j[\"title\"] for i in result[\"metadatas\"] for j in i]\n",
    "all_authors = [j[\"authors\"] for i in result[\"metadatas\"] for j in i]\n",
    "[all_ids] = result[\"ids\"]\n"
   ],
   "id": "65fa82c80be776ce",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.332504Z",
     "start_time": "2025-04-13T19:07:40.328247Z"
    }
   },
   "cell_type": "code",
   "source": "best_article = db.peek(1)",
   "id": "b837ad179ac723bd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.349737Z",
     "start_time": "2025-04-13T19:07:40.346683Z"
    }
   },
   "cell_type": "code",
   "source": "best_article = [best_article[\"metadatas\"][0][\"title\"], best_article[\"metadatas\"][0][\"authors\"], best_article[\"documents\"], best_article[\"ids\"]] ",
   "id": "5a346fd878ea9c1e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.362239Z",
     "start_time": "2025-04-13T19:07:40.359668Z"
    }
   },
   "cell_type": "code",
   "source": "exclude_best = '0' in result[\"ids\"][0]",
   "id": "bd26dff1e4a31276",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.375705Z",
     "start_time": "2025-04-13T19:07:40.372083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if exclude_best:\n",
    "    q = all_ids.index('0')\n",
    "    # best_passage = all_passages[q]\n",
    "    # best_title = all_titles[q]\n",
    "    # best_metadata = result[\"metadatas\"][q]\n",
    "    all_titles = all_titles[:q] + all_titles[q+1:]\n",
    "    all_passages = all_passages[:q] + all_passages[q+1:]\n",
    "    all_ids = all_ids[:q] + all_ids[q+1:]\n",
    "    all_authors = all_authors[:q] + all_authors[q+1:]\n",
    "    \n"
   ],
   "id": "44c397285be8acd8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:07:40.388892Z",
     "start_time": "2025-04-13T19:07:40.386034Z"
    }
   },
   "cell_type": "code",
   "source": "more_article_data = list(zip(all_titles, all_authors, all_passages, all_ids))",
   "id": "b2550796d5582cb1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:18:50.400408Z",
     "start_time": "2025-04-13T19:18:50.394176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_subtopic_articles(subtopics: list, ids: list) -> list:\n",
    "    subtopic_contents = []\n",
    "\n",
    "    for subtopic in subtopics:\n",
    "        results = db.get(\n",
    "        where={\"topic\": subtopic},\n",
    "        limit=10) \n",
    "\n",
    "        idx = None\n",
    "        subtopic_data = None\n",
    "        for i in results[\"ids\"]:\n",
    "            if i not in ids:\n",
    "                idx = results[\"ids\"].index(i)\n",
    "                break\n",
    "\n",
    "        if idx is not None:\n",
    "            article_content = results[\"documents\"][idx]\n",
    "            article_id = results[\"ids\"][idx]\n",
    "            article_title = results[\"metadatas\"][idx][\"title\"]\n",
    "            article_authors = results[\"metadatas\"][idx][\"authors\"]\n",
    "            subtopic_data = article_title, article_authors, article_content, article_id\n",
    "        \n",
    "        if subtopic_data:\n",
    "            subtopic_contents.append(subtopic_data)\n",
    "    return subtopic_contents\n"
   ],
   "id": "dbcd0b0794f9cabc",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:18:50.666660Z",
     "start_time": "2025-04-13T19:18:50.660974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summary(request: str, subtopics: list, ids: list) -> str:\n",
    "    prompt = f'''You are a helpful research assistant. The researcher says: \"{request}\". \n",
    "Here is the data on the main article on the topic:\n",
    " {best_article}\n",
    " Here is the data on a few more articles on the topic: \n",
    " {more_article_data}\n",
    "PLease provide a short summary for each article (a couple of sentences). Please highlight the main article, providing a summary of up to 4 sentences about it. \n",
    "For each article mentioned, be sure to include the article title, authors, and id.  \n",
    "Please don't explain that you got it. \n",
    "'''\n",
    "    \n",
    "  \n",
    "    response = get_model_response(prompt)\n",
    "    if not subtopics: \n",
    "        return response\n",
    "    else: \n",
    "        subtopic_string = \"\\n\".join(subtopics)\n",
    "        keyword_text = (f\"\\n\\n\\n----some helpful keywords/topics may be: {subtopic_string}\")\n",
    "        \n",
    "        \n",
    "        subtopic_articles = get_subtopic_articles(subtopics, ids=ids)\n",
    "        subtopics_prompt = f'''You are a helpful research assistant. The researcher says: \"{request}\". \n",
    "Here are some articles that another assistant suggested to further explore this matter: \n",
    "{subtopic_articles}\n",
    "\n",
    "Please provide a short summary of how the things discussed in the articles add to the concept the researcher was talking about. \n",
    "For each article mentioned, be sure to include the article title, authors, and id.  \n",
    "If no article is relevant, simply return \" \".\n",
    "Please don't explain that you got it. \n",
    "'''\n",
    "        subtopics_response = get_model_response(subtopics_prompt)\n",
    "        return response + keyword_text + \"\\n\\n\\n\" + subtopics_response\n",
    "        "
   ],
   "id": "cbddf6f8f209f609",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:18:57.396467Z",
     "start_time": "2025-04-13T19:18:50.867143Z"
    }
   },
   "cell_type": "code",
   "source": "Markdown(generate_summary(query, st, all_ids))",
   "id": "7f7965035429cfbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Here are summaries of the provided articles:\n\n**Main Article:**\n\n*   **Title:** Killing it with Zero-Shot: Adversarially Robust Novelty Detection\n*   **Authors:** Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, Mohammad Hossein Rohban\n*   **ID:** http://arxiv.org/abs/2501.15271v1\n\nThis paper addresses the vulnerability of novelty detection (ND) algorithms to adversarial attacks. It proposes a method that combines nearest-neighbor algorithms with robust features from ImageNet-pretrained models to enhance the robustness and performance of ND. The results demonstrate significant improvements over state-of-the-art methods under adversarial conditions, establishing a new standard for robust ND. The implementation is publicly available.\n\n**Other Articles:**\n\n*   **Title:** Augmenting Large Language Model Translators via Translation Memories\n*   **Authors:** Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu\n*   **ID:** http://arxiv.org/abs/2305.17367v1\n\nThis paper explores using translation memories (TMs) as prompts for large language models (LLMs) to improve their translation capabilities. The study finds that LLMs can effectively utilize high-quality TM-based prompts, achieving results comparable to state-of-the-art NMT systems.\n\n*   **Title:** Human-in-the-loop Machine Translation with Large Language Model\n*   **Authors:** Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao\n*   **ID:** http://arxiv.org/abs/2310.08908v1\n\nThis paper proposes a human-in-the-loop pipeline for machine translation using LLMs, where human feedback or automatic retrieval is used to guide the LLM's translation process. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance.\n\n*   **Title:** Adaptive Machine Translation with Large Language Models\n*   **Authors:** Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way\n*   **ID:** http://arxiv.org/abs/2301.13294v3\n\nThis paper investigates the use of in-context learning with LLMs to improve real-time adaptive machine translation. The experiments show that LLMs can adapt to in-domain sentence pairs and terminology, surpassing strong encoder-decoder MT systems, especially for high-resource languages.\n\n*   **Title:** Machine Translation for Ge'ez Language\n*   **Authors:** Aman Kassahun Wassie\n*   **ID:** http://arxiv.org/abs/2311.14530v3\n\nThis paper explores various methods to improve machine translation for Ge'ez, a low-resource ancient language, including transfer learning, vocabulary optimization, fine-tuning, and few-shot translation with LLMs. The study finds that GPT-3.5 achieves a reasonable BLEU score with no initial knowledge of Ge'ez, but still lower than the MNMT baseline.\n\n\n\n----some helpful keywords/topics may be: In-context learning for low-resource languages\nZero-shot translation with large language models\nFew-shot translation with large language models\nLLM adaptation to novel languages\nPrompt engineering for machine translation\n\n\nHere's how the provided articles relate to the researcher's interest in LLMs translating unfamiliar languages after being shown vocabulary in the prompt:\n\n*   **Iterative Translation Refinement with Large Language Models** by Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield (http://arxiv.org/abs/2306.03856v2): This paper explores iteratively prompting an LLM to refine translations. While it doesn't directly address translation from completely *unfamiliar* languages based solely on prompt-provided vocabulary, the concept of iterative refinement could be relevant. The LLM could potentially use the initial vocabulary to make a first-pass translation, and then iteratively refine it based on further prompting and context.\n\n*   **Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters** by Daniil Gurgurov, Mareike Hartmann, Simon Ostermann (http://arxiv.org/abs/2407.01406v3): This paper focuses on adapting LLMs to low-resource languages using knowledge graphs. While not exactly the same as translating a completely unfamiliar language from scratch, the techniques used to adapt to low-resource languages could be relevant. The paper explores methods for incorporating external knowledge into LLMs to improve their performance on languages with limited data, which is conceptually similar to providing vocabulary in the prompt.\n\n*   **Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability** by Masaru Yamada (http://arxiv.org/abs/2308.01391v2): This paper investigates how prompt engineering can influence the quality of translations produced by ChatGPT. The idea of providing context and instructions through prompts is directly relevant to the researcher's question. By carefully crafting prompts that include the vocabulary and context of the unfamiliar language, it might be possible to guide the LLM to produce reasonable translations.\n"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eca2c7dd8ee942ed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
