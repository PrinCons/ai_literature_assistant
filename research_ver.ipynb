{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Literature Review Assistant\n",
    "Goal: this notebook provides an LLM \"research assistant\" that can help with exploring the vaguest of questions by providing the relevant papers on the matter. It does not read the papers for you, but it provides a short summary that can be more helpful than the standard abstract when checking out several papers in one sitting. \n",
    "Secondary Goal: this \"assistant\" helps the exploration of sub-topics and alternative keywords; this is helpful when the question is vague or lacks crucial keywords, or when the topic is vast. \n",
    " "
   ],
   "id": "7c0018c510056e83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:35:21.344661Z",
     "start_time": "2025-04-13T19:35:14.506271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import literature_review_assistant\n",
    "from IPython.display import Markdown"
   ],
   "id": "65e9d0f1144c6ba8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l/PycharmProjects/kaggle/literature_review_assistant.py:22: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\n",
      "  from numpy.array_api import result_type\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mliterature_review_assistant\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mIPython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdisplay\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Markdown\n",
      "File \u001B[0;32m~/PycharmProjects/kaggle/literature_review_assistant.py:179\u001B[0m\n\u001B[1;32m    175\u001B[0m                 db\u001B[38;5;241m.\u001B[39madd(documents\u001B[38;5;241m=\u001B[39m[response[article_id][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msummary\u001B[39m\u001B[38;5;124m\"\u001B[39m]], metadatas\u001B[38;5;241m=\u001B[39m[response[article_id][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadatas\u001B[39m\u001B[38;5;124m\"\u001B[39m]],\n\u001B[1;32m    176\u001B[0m                        ids\u001B[38;5;241m=\u001B[39m[article_id])\n\u001B[1;32m    177\u001B[0m                 counter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 179\u001B[0m full_result \u001B[38;5;241m=\u001B[39m generate_search_queries(\u001B[43mquery\u001B[49m)\n\u001B[1;32m    180\u001B[0m r, st \u001B[38;5;241m=\u001B[39m full_result[\u001B[38;5;241m0\u001B[39m], full_result[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    182\u001B[0m generate_db_content(r, st)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'query' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:35:21.405407Z",
     "start_time": "2025-04-13T19:06:54.182399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### set your query \n",
    "query = \"I am looking for a paper where an LLM managed to translate an unfamiliar language after being shown the vocabulary in the prompt \"\n",
    "### set the desired number of subtopics\n",
    "number_of_subtopics = 5"
   ],
   "id": "88854918cc9ebdf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T19:35:21.405600Z",
     "start_time": "2025-04-13T19:18:50.867143Z"
    }
   },
   "cell_type": "code",
   "source": "response = literature_review_assistant.generate_summary(query)",
   "id": "7f7965035429cfbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Here are summaries of the provided articles:\n\n**Main Article:**\n\n*   **Title:** Killing it with Zero-Shot: Adversarially Robust Novelty Detection\n*   **Authors:** Hossein Mirzaei, Mohammad Jafari, Hamid Reza Dehbashi, Zeinab Sadat Taghavi, Mohammad Sabokrou, Mohammad Hossein Rohban\n*   **ID:** http://arxiv.org/abs/2501.15271v1\n\nThis paper addresses the vulnerability of novelty detection (ND) algorithms to adversarial attacks. It proposes a method that combines nearest-neighbor algorithms with robust features from ImageNet-pretrained models to enhance the robustness and performance of ND. The results demonstrate significant improvements over state-of-the-art methods under adversarial conditions, establishing a new standard for robust ND. The implementation is publicly available.\n\n**Other Articles:**\n\n*   **Title:** Augmenting Large Language Model Translators via Translation Memories\n*   **Authors:** Yongyu Mu, Abudurexiti Reheman, Zhiquan Cao, Yuchun Fan, Bei Li, Yinqiao Li, Tong Xiao, Chunliang Zhang, Jingbo Zhu\n*   **ID:** http://arxiv.org/abs/2305.17367v1\n\nThis paper explores using translation memories (TMs) as prompts for large language models (LLMs) to improve their translation capabilities. The study finds that LLMs can effectively utilize high-quality TM-based prompts, achieving results comparable to state-of-the-art NMT systems.\n\n*   **Title:** Human-in-the-loop Machine Translation with Large Language Model\n*   **Authors:** Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao\n*   **ID:** http://arxiv.org/abs/2310.08908v1\n\nThis paper proposes a human-in-the-loop pipeline for machine translation using LLMs, where human feedback or automatic retrieval is used to guide the LLM's translation process. The results demonstrate the effectiveness of the pipeline in tailoring in-domain translations and improving translation performance.\n\n*   **Title:** Adaptive Machine Translation with Large Language Models\n*   **Authors:** Yasmin Moslem, Rejwanul Haque, John D. Kelleher, Andy Way\n*   **ID:** http://arxiv.org/abs/2301.13294v3\n\nThis paper investigates the use of in-context learning with LLMs to improve real-time adaptive machine translation. The experiments show that LLMs can adapt to in-domain sentence pairs and terminology, surpassing strong encoder-decoder MT systems, especially for high-resource languages.\n\n*   **Title:** Machine Translation for Ge'ez Language\n*   **Authors:** Aman Kassahun Wassie\n*   **ID:** http://arxiv.org/abs/2311.14530v3\n\nThis paper explores various methods to improve machine translation for Ge'ez, a low-resource ancient language, including transfer learning, vocabulary optimization, fine-tuning, and few-shot translation with LLMs. The study finds that GPT-3.5 achieves a reasonable BLEU score with no initial knowledge of Ge'ez, but still lower than the MNMT baseline.\n\n\n\n----some helpful keywords/topics may be: In-context learning for low-resource languages\nZero-shot translation with large language models\nFew-shot translation with large language models\nLLM adaptation to novel languages\nPrompt engineering for machine translation\n\n\nHere's how the provided articles relate to the researcher's interest in LLMs translating unfamiliar languages after being shown vocabulary in the prompt:\n\n*   **Iterative Translation Refinement with Large Language Models** by Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield (http://arxiv.org/abs/2306.03856v2): This paper explores iteratively prompting an LLM to refine translations. While it doesn't directly address translation from completely *unfamiliar* languages based solely on prompt-provided vocabulary, the concept of iterative refinement could be relevant. The LLM could potentially use the initial vocabulary to make a first-pass translation, and then iteratively refine it based on further prompting and context.\n\n*   **Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters** by Daniil Gurgurov, Mareike Hartmann, Simon Ostermann (http://arxiv.org/abs/2407.01406v3): This paper focuses on adapting LLMs to low-resource languages using knowledge graphs. While not exactly the same as translating a completely unfamiliar language from scratch, the techniques used to adapt to low-resource languages could be relevant. The paper explores methods for incorporating external knowledge into LLMs to improve their performance on languages with limited data, which is conceptually similar to providing vocabulary in the prompt.\n\n*   **Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT's Customizability** by Masaru Yamada (http://arxiv.org/abs/2308.01391v2): This paper investigates how prompt engineering can influence the quality of translations produced by ChatGPT. The idea of providing context and instructions through prompts is directly relevant to the researcher's question. By carefully crafting prompts that include the vocabulary and context of the unfamiliar language, it might be possible to guide the LLM to produce reasonable translations.\n"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Markdown(response)",
   "id": "eca2c7dd8ee942ed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
